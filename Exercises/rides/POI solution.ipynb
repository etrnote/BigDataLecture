{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_212\"\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('poi_exercise') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
    "    .config('spark.driver.memory','10G') \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>poi_exercise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdb0c003610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "\n",
    "def haversine_formula(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Return the distance between two coordinates, in meters.\n",
    "    \"\"\"\n",
    "    lat1 = math.pi / 180.0 * lat1\n",
    "    lon1 = math.pi / 180.0 * lon1\n",
    "    lat2 = math.pi / 180.0 * lat2\n",
    "    lon2 = math.pi / 180.0 * lon2\n",
    "    radius = 6371  # km\n",
    "    meters = 1000\n",
    "\n",
    "    # Use the haversine formula:\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = math.pow(math.sin(dlat/2),2) + math.cos(lat1) * math.cos(lat2) * math.pow(math.sin(dlon/2),2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    meters = radius * c * meters\n",
    "    return meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+\n",
      "|       Date/Time|    Lat|     Lon|  Base|\n",
      "+----------------+-------+--------+------+\n",
      "|4/1/2014 0:11:00| 40.769|-73.9549|B02512|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|\n",
      "+----------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+----------+----+----------------+\n",
      "|      Lat|       Lon|Type|            Name|\n",
      "+---------+----------+----+----------------+\n",
      "|40.640761|-73.699404|cafe|       Starbucks|\n",
      "|41.789708|-87.601708|cafe|Starbucks Coffee|\n",
      "|29.911807|-95.685513|cafe|       Starbucks|\n",
      "|29.845172|-95.646238|cafe|       Starbucks|\n",
      "|40.529512|-74.540861|bank|           Chase|\n",
      "+---------+----------+----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rides = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/rides_small_ds.csv\")\n",
    "df_poi = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/poi_small_ds.csv\").withColumn('Lat', F.col(\"Lat\").cast(\"double\"))\n",
    "\n",
    "df_rides.show(5)\n",
    "df_poi.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------------+\n",
      "|       arrival_time|    Lat|     Lon|arrival_hour|\n",
      "+-------------------+-------+--------+------------+\n",
      "|2014-04-01 06:00:00|40.7507|-73.9703|           6|\n",
      "|2014-04-01 06:01:00|40.7337|-73.9979|           6|\n",
      "|2014-04-01 06:02:00|40.7236|-74.0111|           6|\n",
      "|2014-04-01 06:03:00|40.7151|-74.0464|           6|\n",
      "|2014-04-01 06:06:00|40.7701|-73.9588|           6|\n",
      "+-------------------+-------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format the date and extract only the event that are on the morning or evening range\n",
    "df_morning_event_rides = df_rides\\\n",
    "                          .select(F.to_timestamp(F.col(\"Date/Time\"), \"M/d/y H:mm:ss\").alias(\"arrival_time\"), F.col(\"Lat\"), F.col(\"Lon\"))\\\n",
    "                          .withColumn(\"arrival_hour\", F.hour(F.col(\"arrival_time\")))\\\n",
    "                          .where(\"(arrival_hour between 6 and 11) or (arrival_hour between 17 and 23)\")\n",
    "df_morning_event_rides.show(5)\n",
    "\n",
    "df_morning_event_rides = df_morning_event_rides.filter(F.col('Lat').isNotNull() & F.col('Lon').isNotNull())\n",
    "df_poi = df_poi.filter(F.col('Lat').isNotNull() & F.col('Lon').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------------+---------+----------+----+-----+------------------+\n",
      "|       arrival_time|    Lat|     Lon|arrival_hour|      Lat|       Lon|Type| Name|          distance|\n",
      "+-------------------+-------+--------+------------+---------+----------+----+-----+------------------+\n",
      "|2014-04-02 07:30:00|40.7347|-73.9797|           7|40.737221|-73.978682|bank|Chase| 293.1508966472016|\n",
      "|2014-04-02 07:44:00|40.7377|-73.9802|           7|40.737221|-73.978682|bank|Chase|138.54379820956495|\n",
      "|2014-04-02 07:52:00|40.7364|-73.9797|           7|40.737221|-73.978682|bank|Chase|125.26236330434024|\n",
      "|2014-04-02 08:07:00|40.7369|-73.9789|           8|40.737221|-73.978682|bank|Chase| 40.14211725233168|\n",
      "|2014-04-02 08:20:00|40.7349|-73.9772|           8|40.737221|-73.978682|bank|Chase|286.70291962886296|\n",
      "+-------------------+-------+--------+------------+---------+----------+----+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution #1 - Use a UDF for the haversine_formula - Takes ~45 seconds\n",
    "from pyspark.sql import types as T\n",
    "udf_haversine_formula = F.udf(haversine_formula)\n",
    "\n",
    "# Attach to each POI all drop offs and pickups that are less than 100 meters apart, using the UDF.\n",
    "# df_combined = df_morning_event_rides\\\n",
    "#                 .crossJoin(df_poi)\\\n",
    "#                 .where(udf_haversine_formula(df_morning_event_rides[\"Lat\"], df_morning_event_rides[\"Lon\"], df_poi[\"Lat\"], df_poi[\"Lon\"]) < 300)\\\n",
    "#                 .cache()\n",
    "# df_combined.show(5)\n",
    "\n",
    "df_morning_event_rides = df_morning_event_rides.filter(F.col('Lat').isNotNull() & F.col('Lon').isNotNull())\n",
    "df_poi = df_poi.filter(F.col('Lat').isNotNull() & F.col('Lon').isNotNull())\n",
    "\n",
    "df_combined = df_morning_event_rides\\\n",
    "                .crossJoin(df_poi)\\\n",
    "                .withColumn('distance', udf_haversine_formula(df_morning_event_rides[\"Lat\"], df_morning_event_rides[\"Lon\"], df_poi[\"Lat\"], df_poi[\"Lon\"]))\\\n",
    "                .filter(F.col('distance') < 300)\\\n",
    "                .cache()\n",
    "df_combined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------------+------------------+-------------------+----+-----+-----------------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|       arrival_time|    Lat|     Lon|arrival_hour|     lat_hav_rides|      lon_hav_rides|Type| Name|      lat_hav_poi|        lon_hav_poi|                dlon|                dlat|                   a|                   c|          distance|\n",
      "+-------------------+-------+--------+------------+------------------+-------------------+----+-----+-----------------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|2014-04-01 07:46:00|40.7396| -73.981|           7| 0.711040155945482| -1.291212033917925|bank|Chase|0.710998634562577|-1.2911715771858638|4.045673206110045E-5|-4.15213829050298...|6.659223914694289...|5.161094425089494E-5|328.81332582245165|\n",
      "|2014-04-01 22:42:00|40.7401|-73.9792|          22|0.7110488825917418|-1.2911806179913892|bank|Chase|0.710998634562577|-1.2911715771858638|9.040805525328821E-6|-5.02480291648677E-5|6.429473005488811...|5.071281103179648E-5| 323.0913190835754|\n",
      "|2014-04-02 07:30:00|40.7347|-73.9797|           7|0.7109546348121341| -1.291189344637649|bank|Chase|0.710998634562577|-1.2911715771858638| 1.77674517851667E-5|4.399975044289128E-5|5.29306530389711E-10|4.601332548221654E-5| 293.1508966472016|\n",
      "|2014-04-02 07:41:00|40.7377|-73.9827|           7| 0.711006994689694|-1.2912417045152087|bank|Chase|0.710998634562577|-1.2911715771858638| 7.01273293448601E-5|-8.36012711702416...|7.233322533845701...|5.378967386251652...|342.69401217809275|\n",
      "|2014-04-02 07:44:00|40.7377|-73.9802|           7| 0.711006994689694|-1.2911980712839088|bank|Chase|0.710998634562577|-1.2911715771858638|2.649409804500457...|-8.36012711702416...|1.182221839654620...|2.174600505565295E-5|138.54379820956495|\n",
      "+-------------------+-------+--------+------------+------------------+-------------------+----+-----+-----------------+-------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution #2 - Implement the Haversine Formula with SparkSQL's functions to improve the performance - Takes ~2 seconds\n",
    "# To understand why UDFs in Python are not always the best option, read here: https://stackoverflow.com/questions/38296609/spark-functions-vs-udf-performance\n",
    "\n",
    "pi = math.pi\n",
    "radius = 6371  # km\n",
    "meters = 1000\n",
    "\n",
    "# Prepare the two main dataframes\n",
    "df_morning_event_rides_hav = df_morning_event_rides\\\n",
    "                              .withColumn(\"lat_hav_rides\",(pi / 180.0 * F.col(\"Lat\")))\\\n",
    "                              .withColumn(\"lon_hav_rides\", (pi / 180.0 * F.col(\"Lon\")))\n",
    "\n",
    "df_poi_hav = df_poi\\\n",
    "              .withColumn(\"lat_hav_poi\", (pi / 180.0 * F.col(\"Lat\")))\\\n",
    "              .withColumn(\"lon_hav_poi\", (pi / 180.0 * F.col(\"Lon\")))\n",
    "\n",
    "# Crossjoin and apply haversine formula\n",
    "df_combined = df_morning_event_rides_hav\\\n",
    "                  .crossJoin(df_poi_hav.drop(\"Lat\", \"Lon\"))\\\n",
    "                  .withColumn('dlon', F.col(\"lon_hav_poi\") - F.col(\"lon_hav_rides\"))\\\n",
    "                  .withColumn('dlat', F.col(\"lat_hav_poi\") - F.col(\"lat_hav_rides\"))\\\n",
    "                  .withColumn('a', F.pow(F.sin(F.col('dlat')/2),2)+ F.cos(F.col(\"lat_hav_rides\"))*F.cos(F.col(\"lat_hav_poi\"))*F.pow((F.sin(F.col('dlon')/2)),2))\\\n",
    "                  .withColumn('c', 2 * F.atan2(F.sqrt(F.col('a')), F.sqrt(1-F.col('a'))))\\\n",
    "                  .withColumn('distance', radius * meters * F.col('c'))\\\n",
    "                  .filter(F.col('distance') < 500)\\\n",
    "                  .cache()\n",
    "df_combined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>arrival_time</th><th>Lat</th><th>Lon</th><th>arrival_hour</th><th>Lat</th><th>Lon</th><th>Type</th><th>Name</th><th>distance</th></tr>\n",
       "<tr><td>2014-04-02 07:30:00</td><td>40.7347</td><td>-73.9797</td><td>7</td><td>40.737221</td><td>-73.978682</td><td>bank</td><td>Chase</td><td>293.1508966472016</td></tr>\n",
       "<tr><td>2014-04-02 07:44:00</td><td>40.7377</td><td>-73.9802</td><td>7</td><td>40.737221</td><td>-73.978682</td><td>bank</td><td>Chase</td><td>138.54379820956495</td></tr>\n",
       "<tr><td>2014-04-02 07:52:00</td><td>40.7364</td><td>-73.9797</td><td>7</td><td>40.737221</td><td>-73.978682</td><td>bank</td><td>Chase</td><td>125.26236330434024</td></tr>\n",
       "<tr><td>2014-04-02 08:07:00</td><td>40.7369</td><td>-73.9789</td><td>8</td><td>40.737221</td><td>-73.978682</td><td>bank</td><td>Chase</td><td>40.14211725233168</td></tr>\n",
       "<tr><td>2014-04-02 08:20:00</td><td>40.7349</td><td>-73.9772</td><td>8</td><td>40.737221</td><td>-73.978682</td><td>bank</td><td>Chase</td><td>286.70291962886296</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+-------+--------+------------+---------+----------+----+-----+------------------+\n",
       "|       arrival_time|    Lat|     Lon|arrival_hour|      Lat|       Lon|Type| Name|          distance|\n",
       "+-------------------+-------+--------+------------+---------+----------+----+-----+------------------+\n",
       "|2014-04-02 07:30:00|40.7347|-73.9797|           7|40.737221|-73.978682|bank|Chase| 293.1508966472016|\n",
       "|2014-04-02 07:44:00|40.7377|-73.9802|           7|40.737221|-73.978682|bank|Chase|138.54379820956495|\n",
       "|2014-04-02 07:52:00|40.7364|-73.9797|           7|40.737221|-73.978682|bank|Chase|125.26236330434024|\n",
       "|2014-04-02 08:07:00|40.7369|-73.9789|           8|40.737221|-73.978682|bank|Chase| 40.14211725233168|\n",
       "|2014-04-02 08:20:00|40.7349|-73.9772|           8|40.737221|-73.978682|bank|Chase|286.70291962886296|\n",
       "+-------------------+-------+--------+------------+---------+----------+----+-----+------------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-------------+\n",
      "|                Name|            type|num_of_visits|\n",
      "+--------------------+----------------+-------------+\n",
      "|           Starbucks|            cafe|          982|\n",
      "|               Chase|            bank|          610|\n",
      "|         Duane Reade|        pharmacy|          592|\n",
      "|    Starbucks Coffee|            cafe|          378|\n",
      "|            Citibank|            bank|          351|\n",
      "|           Starbucks|         toilets|           17|\n",
      "|     Municipal Lot D|         parking|            5|\n",
      "|     Municipal Lot B|         parking|            5|\n",
      "|     Municipal Lot G|         parking|            4|\n",
      "|                 CVS|        pharmacy|            3|\n",
      "|Saint Francis Church|place_of_worship|            2|\n",
      "|Rose of Sharon Ho...|place_of_worship|            2|\n",
      "| private parking lot|         parking|            1|\n",
      "+--------------------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the most popular POIs\n",
    "\n",
    "# Option 1 - With SparkSQL\n",
    "df_sorted = df_combined.groupby('Name', 'type').count().sort(\"count\", ascending=False).withColumnRenamed('count', 'num_of_visits')\n",
    "\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 - With SparkSQL Context\n",
    "df_combined.createOrReplaceTempView(\"combined\")\n",
    "df_sorted = spark.sql(\"select Name, count(*) as num_of_visits from combined group by Name order by 2 desc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
